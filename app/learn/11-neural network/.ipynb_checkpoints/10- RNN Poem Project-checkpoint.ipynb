{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c63dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time as TM\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d470b827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36711, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_csv('../../../datasets/sheer/amir_norm.txt', names=['body'], encoding='utf-8')\n",
    "# DF = DF.iloc[:1000,:]\n",
    "DF['body'] = DF['body'].str.strip()\n",
    "DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f40120",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = sorted(set( DF.loc[:,'body'].str.split('').sum() ))\n",
    "vocabs = list(pd.DataFrame([v for v in vocabs]).dropna()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42c6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_size = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb6735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2index = {u:i for i,u in enumerate(vocabs)}\n",
    "index2char = np.array(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79265f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 16:27:26.527254: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-25 16:27:26.527885: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-25 16:27:26.528033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: masoud-Aspire-V3-571G\n",
      "2022-12-25 16:27:26.528072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: masoud-Aspire-V3-571G\n",
      "2022-12-25 16:27:26.528265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2022-12-25 16:27:26.528415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.157.0\n"
     ]
    }
   ],
   "source": [
    "DF_ENCODED = DF['body'].apply(lambda x: [char2index[c] for c in x] ).sum()\n",
    "DF_ENCODED_TENSOR = Dataset.from_tensor_slices(DF_ENCODED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b605674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = DF_ENCODED_TENSOR.batch(150, drop_remainder=True)\n",
    "def preproces(batch):\n",
    "    input_text = batch[:-1]\n",
    "    target_text = batch[1:]\n",
    "    return input_text, target_text\n",
    "sequences = sequences.map(preproces)\n",
    "dataset = sequences.batch(60,drop_remainder=True)\n",
    "batch_input_shape = (dataset.element_spec[0].shape[0], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25059c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 4  3  1 ... 26  1 12]\n",
      " [14 24 18 ... 30 35 10]\n",
      " [12 13 25 ... 16 10  1]\n",
      " ...\n",
      " [33  5  2 ...  3 22 14]\n",
      " [ 1 27 25 ... 26  1 25]\n",
      " [18 22  3 ...  3 30 27]], shape=(60, 149), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for input_text, target_text in dataset.take(1):\n",
    "    print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1d35ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vacabs_size, embedding_dim, batch_input_shape, rnn_unit_count, batch_size,epochs,fitCompile=True):\n",
    "    model = tf.keras.Sequential([\n",
    "        #tf.keras.layers.Embedding(vocabs_size, embedding_dim, batch_input_shape=batch_input_shape),\n",
    "        # tf.keras.layers.GRU(rnn_unit_count, return_sequences=True, stateful=True),\n",
    "\n",
    "        tf.keras.layers.Embedding(vocabs_size, embedding_dim),\n",
    "        tf.keras.layers.GRU(rnn_unit_count, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vacabs_size)\n",
    "    ])\n",
    "    \n",
    "    def loss_f(labels, logits):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "     \n",
    "    if fitCompile:\n",
    "        model.compile(optimizer='adam',loss=loss_f)\n",
    "        checkpoints= tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f'{os.path.abspath(path=\".\")}/poem_checkpointshafez/amir_norm/amir_norm',\n",
    "            save_weights_only=True,\n",
    "#             save_freq = 10\n",
    "        )\n",
    "        history = model.fit(dataset, epochs=epochs, callbacks=[checkpoints])\n",
    "        return model, history\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f978146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "129/129 [==============================] - 253s 2s/step - loss: 2.8805\n",
      "Epoch 2/50\n",
      "129/129 [==============================] - 273s 2s/step - loss: 2.5175\n",
      "Epoch 3/50\n",
      "129/129 [==============================] - 288s 2s/step - loss: 2.4040\n",
      "Epoch 4/50\n",
      "129/129 [==============================] - 299s 2s/step - loss: 2.3019\n",
      "Epoch 5/50\n",
      "129/129 [==============================] - 299s 2s/step - loss: 2.2235\n",
      "Epoch 6/50\n",
      "129/129 [==============================] - 295s 2s/step - loss: 2.1535\n",
      "Epoch 7/50\n",
      "129/129 [==============================] - 296s 2s/step - loss: 2.0889\n",
      "Epoch 8/50\n",
      "129/129 [==============================] - 295s 2s/step - loss: 2.0314\n",
      "Epoch 9/50\n",
      "129/129 [==============================] - 296s 2s/step - loss: 1.9817\n",
      "Epoch 10/50\n",
      "129/129 [==============================] - 301s 2s/step - loss: 1.9389\n",
      "Epoch 11/50\n",
      "129/129 [==============================] - 318s 2s/step - loss: 1.9016\n",
      "Epoch 12/50\n",
      "129/129 [==============================] - 323s 2s/step - loss: 1.8686\n",
      "Epoch 13/50\n",
      "129/129 [==============================] - 289s 2s/step - loss: 1.8390\n",
      "Epoch 14/50\n",
      "129/129 [==============================] - 286s 2s/step - loss: 1.8120\n",
      "Epoch 15/50\n",
      "129/129 [==============================] - 285s 2s/step - loss: 1.7871\n",
      "Epoch 16/50\n",
      "129/129 [==============================] - 272s 2s/step - loss: 1.7637\n",
      "Epoch 17/50\n",
      "129/129 [==============================] - 281s 2s/step - loss: 1.7414\n",
      "Epoch 18/50\n",
      "129/129 [==============================] - 282s 2s/step - loss: 1.7198\n",
      "Epoch 19/50\n",
      "129/129 [==============================] - 284s 2s/step - loss: 1.6989\n",
      "Epoch 20/50\n",
      "129/129 [==============================] - 284s 2s/step - loss: 1.6783\n",
      "Epoch 21/50\n",
      "129/129 [==============================] - 282s 2s/step - loss: 1.6578\n",
      "Epoch 22/50\n",
      "129/129 [==============================] - 282s 2s/step - loss: 1.6374\n",
      "Epoch 23/50\n",
      "129/129 [==============================] - 282s 2s/step - loss: 1.6170\n",
      "Epoch 24/50\n",
      "129/129 [==============================] - 269s 2s/step - loss: 1.5971\n",
      "Epoch 25/50\n",
      "129/129 [==============================] - 281s 2s/step - loss: 1.5789\n",
      "Epoch 26/50\n",
      "129/129 [==============================] - 281s 2s/step - loss: 1.5591\n",
      "Epoch 27/50\n",
      "129/129 [==============================] - 279s 2s/step - loss: 1.5380\n",
      "Epoch 28/50\n",
      "129/129 [==============================] - 281s 2s/step - loss: 1.5183\n",
      "Epoch 29/50\n",
      "129/129 [==============================] - 280s 2s/step - loss: 1.4960\n",
      "Epoch 30/50\n",
      "129/129 [==============================] - 283s 2s/step - loss: 1.4724\n",
      "Epoch 31/50\n",
      "129/129 [==============================] - 279s 2s/step - loss: 1.4508\n",
      "Epoch 32/50\n",
      "129/129 [==============================] - 267s 2s/step - loss: 1.4346\n",
      "Epoch 33/50\n",
      "129/129 [==============================] - 278s 2s/step - loss: 1.4221\n",
      "Epoch 34/50\n",
      "129/129 [==============================] - 280s 2s/step - loss: 1.4067\n",
      "Epoch 35/50\n",
      "129/129 [==============================] - 279s 2s/step - loss: 1.3925\n",
      "Epoch 36/50\n",
      "129/129 [==============================] - 282s 2s/step - loss: 1.3833\n",
      "Epoch 37/50\n",
      "129/129 [==============================] - 279s 2s/step - loss: 1.3771\n",
      "Epoch 38/50\n",
      "129/129 [==============================] - 281s 2s/step - loss: 1.3743\n",
      "Epoch 39/50\n",
      "129/129 [==============================] - 280s 2s/step - loss: 1.3698\n",
      "Epoch 40/50\n",
      "129/129 [==============================] - 282s 2s/step - loss: 1.3617\n",
      "Epoch 41/50\n",
      "129/129 [==============================] - 267s 2s/step - loss: 1.3561\n",
      "Epoch 42/50\n",
      "129/129 [==============================] - 277s 2s/step - loss: 1.3524\n",
      "Epoch 43/50\n",
      "129/129 [==============================] - 278s 2s/step - loss: 1.3498\n",
      "Epoch 44/50\n",
      "129/129 [==============================] - 277s 2s/step - loss: 1.3464\n",
      "Epoch 45/50\n",
      "129/129 [==============================] - 278s 2s/step - loss: 1.3387\n",
      "Epoch 46/50\n",
      "129/129 [==============================] - 279s 2s/step - loss: 1.3320\n",
      "Epoch 47/50\n",
      "129/129 [==============================] - 278s 2s/step - loss: 1.3265\n",
      "Epoch 48/50\n",
      "129/129 [==============================] - 277s 2s/step - loss: 1.3205\n",
      "Epoch 49/50\n",
      "129/129 [==============================] - 278s 2s/step - loss: 1.3145\n",
      "Epoch 50/50\n",
      "129/129 [==============================] - 278s 2s/step - loss: 1.3071\n"
     ]
    }
   ],
   "source": [
    "model, history = build_model(vocabs_size,120,batch_input_shape,512,20,50)\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d35022a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(vocabs_size,250,batch_input_shape,1024,20,30,False)\n",
    "# model.load_weights(\n",
    "#     tf.train.latest_checkpoint(f'{os.path.abspath(path=\".\")}/poem_checkpointshafez/hafez')\n",
    "# )\n",
    "# model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f733cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ببآابب  ااا اااآ ااآ اآ  ب بآ\n",
      "رثتنصاصدنوىقىبباقررسرودخرعص  ررغصمحا\n",
      "سدپىبظییپىبیثىضحخىتحامنآتلیفىپتذپووص\n",
      "ذچجحنغغصسهیقکقىحععععیذتذثرقژخعسحچپآ\n",
      "دبظکضحگاقوگخصکووشصضلگجضنخگ ژدپدکدهپ\n",
      "شکشغذزشلر لممکغىرشرجغظآظجرهجعظنطجصقگ\n",
      "فگحلسآفپححپفازلیگحدذدفىطسپفژکحاثفضمح\n",
      "غچخهعنوضزذش صبآگپنارلژتىبهتم وبعوشزد\n",
      "طظذپغدبضرذغششىصچطفآطگبرفراسبآطذکصدغت\n",
      "جکلتقوزژلسطصمچوطضژضوژمبدوچشجضراهزا\n"
     ]
    }
   ],
   "source": [
    "first_string = 'مسعود'\n",
    "input_eval = [char2index[c] for c in first_string]\n",
    "input_eval = tf.expand_dims(input_eval,0)\n",
    "\n",
    "num_generate = 10\n",
    "generated_text = []\n",
    "model.reset_states()\n",
    "\n",
    "for i in range(num_generate):\n",
    "    predicted = model.predict(input_eval,verbose=False)\n",
    "#     predicted = tf.random.categorical(predicted[0], num_samples=1)\n",
    "    predicted = predicted.argmax(axis=1).reshape(-1,1)\n",
    "    predicted_id = tf.squeeze(predicted, axis=-1)[-1].numpy()\n",
    "    input_eval = np.append(input_eval[0],predicted_id)[1:]\n",
    "    predicted = ''.join(index2char[np.reshape(predicted,-1)])\n",
    "    input_eval = [char2index[c] for c in predicted]\n",
    "    input_eval = tf.expand_dims(input_eval,0)\n",
    "    print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c1bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470891f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
